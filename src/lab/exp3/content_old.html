<!DOCTYPE html>
<head></head>
<h1 id="main-title">
Linear Perceptron
</h1>

<div id="introduction">
    <h2>Introduction </h2>
    <p> Linear perceptron is one of the simplest learning algorithms for a two-class classifier.
    Given a set of data points in d-dimensions, belonging to two classes, <i>C1</i> and <i>C2</i>,
    the algorithm tries to find a linear separating hyper-plane between the samples of the
    two classes. If the samples are in one, two or three dimensions, the separating hyperplane
    would be a point, line or a plane respectively. The specific algorithm that we look into is a special
    case of a class of algorithms that uses gradient descent on a carefully defined objective function
    to arrive at a solution.</p>

</div>
<div id="theory">
    <h2>Theory</h2>
    <p>Assume that the samples of the
    two classes are linearly separable in the feature space.  i.e., there exists a plane <b>w.x</b>+<i>w0 = 0</i>
    such that all samples belonging to the first class are on one side of the plane, and all samples of the
    second class are on the opposite side. If such planes exist, the goal of the perceptron algorithm
    is to learn any one such plane, given the data points. Once we learn the plane, it will be easy to classify
    new points in the future, as the points on one side of the plane will result in a positive value for
    <b>w.x</b>+<i>w0</i>, while points on the other side will give a negative value.</p>

    <h3>Special Case: One dimensional feature vector</h3>
     <p> To understand the working of the algorithm, we first take the simplest case, where the
    samples are represented by a single feature (one-dimensional).</p>

</div>

<div id="objective">
    <h2> Objective </h2>
    <p>The high level goals of the experiment are:
        <ol>
            <li> <b>1:</b> To understand the working of linear perceptron learning algorithm.</li>
            <li> <b>2:</b> To understand the effect of various parameters on the learning rate and convergence of the algorithm.</li>
            <li> <b>3:</b> To understand the effect of data distribution on learnability of the algorithm.</li>
            <li> <b>4:</b> To understand the implication of learning a constrained plane in <i>d+1</i>-dimensional space.</li>
        </ol>
    </p>
</div>


<div id="procedure">
    <h2> Procedure </h2>
    <p>
    .</p>
    <h3>Stage 1:</h3>
    <ol>
        <li>Create a linearly separable dataset by clicking on the plane to add points to classes 1 and 2. </li>
        <li> Click start to initialize the weight vector (you may re-initialize by clicking on it again)</li>
        <li> Click the step button to see the stepping through the algorithm.</li>
        <li> Note number of iterations for convergence.</i>
    </ol>

    <h3>Stage 2:</h3>
    <p> Repeat the above procedure for the different datasets with varying amount of separation between them.
    Note down your observations.

    <h3>Stage 3:</h3>
    <p> Study the effect of varying the decay rate with different amounts of separability.

</div>

<div id="experiment">
    <h2> Experiment </h2>
   <a href="../experiment/Exp3.jnlp">Click here to open the Experiment</a>
</div>

<div id="quizzes">
    <h2>Quiz</h2>
    <p>
    <img src="images/perceptron.png"><br/><br/>Data points are: Negative: (1, 1) (3, 1) (1, 4) Positive: (2, 4) (3, 3) (5, 1). Data points are classified as either +1 or -1. An unknown point is located at (2, 3)(Q 1-4). 
<br/><br/>
    <b>Q1. </b>Assume that the points are examined in the order given above, starting with the negative points and then the positive points. Simulate one iteration of the perceptron algorithm with a learning rate of 0.5 and an initial weight vector of (-15 5 3).<br/><br/>
    <b>Q2. </b> What is the equation of this line using the final weights from Question 1.<br/><br/>
    <b>Q3. </b> Is this line a linear separator of the data? <br/><br/>
    <b>Q4. </b> Using this line, what would be the predicted class of the unknown point (2, 3)? What is the margin of this point using the predicted class?<br/><br/>
<b>Q5. </b> For which kind of problem is the Adaline algorithm the best ?<br/><br/>
<b>Q6. </b> For which kind of problem is the Backpropagation algorithm the best ?<br/><br/>
<b>Q7. </b> For which kind of problem is the Perceptron algorithm the best ?<br/><br/>
<b>Q8. </b> What would happen if the output function in a multi-layer perceptron would be omitted; i.e. if the output would simply be the weighted sum of all input signals? Why is that simpler output not normally used in MLPs although it would simplify and accelerate the calculations for the backpropogation algorithm? <br/><br/>
<b>Q9. </b> Consider a classical xor example(two same inputs give 1 and diffrent inputs give 0). How does the decision line look like? Is a
simple, one-layer perceptron able to realize this function?
Show by symbolic representations that the simple one-layer perceptron with two inputs
x1 and x2 as well as a threshold (theta) cannot realize the xor function. <br/><br/>
    
    </p>
</div>

<div id="readings">
    <h2> Further Readings </h2>
    <ul>
        <li><b>*</b>Article on the  <a href="http://en.wikipedia.org/wiki/Perceptron">Perceptron</a>, at Wikipedia
        </li>
        <li><b>*</b> Article on the <a href="http://en.wikipedia.org/wiki/Multilayer_perceptron">MultiLayer Perceptron</a> at Wikipedia
        </li>
        <li><b>*</b> Lecture on the <a href="http://www.cs.cmu.edu/~avrim/ML09/lect0126.pdf">Perceptron Algorithm</a> at CMU
        </li>
        <li><b>*</b> Learning Applet on the <a href="http://lcn.epfl.ch/tutorial/english/perceptron/html/index.html">Perceptron Learning Applet</a> 
        </li>
        <li><b>*</b> Perceptron tutorials on <a href="http://realintelligence.net/tutorials.php?category=ann&tutName=perceptron">Real Intelligence</a> 
        </li>
        <li><b>*</b> Tutorials on multilayer Perceptron <a href="http://www.cs.sun.ac.za/~kroon/courses/machine_learning/lecture5/mlp.pdf">MultiLayer Perceptron</a> 
        </li>

    </ul>
</div>
