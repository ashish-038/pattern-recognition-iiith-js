<!DOCTYPE html>
<head></head>
<h1 id="main-title">
MLE: Learning the Classifier from Data
</h1>

<div id="introduction">
    <h2>Introduction </h2>
    <p> In the previous experiment, we learned about how to generate a classifier, given
    models for each of the classes, using the Bayes Rule. The next goal is to learn such
    a classifier, automatically, given a set of labelled samples from each of the possible
    classes.</p>
</div>

<div id="theory">
    <h2>Theory</h2>

Consider the following quote from a 2000 article in the Economist on the Bayesian Approach 
[<a href="http://www.ai.mit.edu/%7emurphyk/Bayes/economist.html">link</a><span style="color: rgb(0, 0, 0);">]:</span>

<p><i>"The essence of the Bayesian approach is to provide a mathematical rule explaining 
how you should change your existing beliefs in the light of new evidence. In
other words, it allows us to combine new data with their existing knowledge or
expertise. The canonical example is to imagine that a precocious newborn observes 
his first sunset, and wonders whether the sun will rise again or not. He assigns 
equal prior probabilities to both possible outcomes, and represents this by placing
one white and one black marble into a bag. The following day, when
the sun rises, the child places another white marble in the bag. The
probability that a marble plucked randomly from the bag will be white (i.e., the 
child's degree of belief in future sunrises) has thus gone from a half to two-thirds. 
After sunrise the next day, the child adds another white marble, and the probability 
(and thus the degree of belief) goes from two-thirds to three-quarters. And so on. 
Gradually, the initial belief that the sun is just as likely as not to rise each
morning is modified to become a near-certainty that the sun will always
rise." </i></p>

<p> In terms of classification, the Bayes theorem allows us to combine prior probabilities,
along with observed evidence to arrive at the posterior probability. More or less, conditional
probabilities represent the probability of an event occurring given evidence. To better
understand, Bayes Theorem can be derived from the joint probability of
A and B (i.e. <i>P</i>(<i>A,B</i>)) as
follows:</span></p>
<br>
<p style="text-align: center;"><span style="color: rgb(0, 0, 0);"> <span class="WPCharBoxWrapper" style="width: 217px;"><span class="WPCharBox" style="border: medium none ;"><img src="images/ole.gif" alt="ole.gif" border="0" height="101" width="217"></span></span> </span></p>
<br>

<p><span style="color: rgb(0, 0, 0);">where
</span><span class="serifequation"><span style="color: rgb(0, 0, 0);"><i><span style="font-weight: bold;">P</span></i><span style="font-weight: bold;">(<i>A</i>|<i>B</i>)</span></span></span><span style="color: rgb(0, 0, 0);"> is referred to as the <i>posterior</i>;
<i><span style="font-weight: bold;">P</span></i><span style="font-weight: bold;">(<i>B</i>|<i>A</i>)</span>
is known as the <i>likelihood</i>, <i><span style="font-weight: bold;">P</span></i><span style="font-weight: bold;">(<i>A</i>)</span>
is the <i>prior</i>
and <i><span style="font-weight: bold;">P</span></i><span style="font-weight: bold;">(<i>B</i>)</span>
is generally the <i>evidence</i> and is used as a scaling factor. Therefore, it is handy to
remember Bayes Rule as:</span></p>

<p style="text-align: center;"><span style="color: rgb(0, 0, 0);"> <span class="WPCharBoxWrapper" style="width: 237px;"><span class="WPCharBox" style="border: medium none ;"><img src="images/ole1.gif" alt="ole1.gif" border="0" height="47" width="237"></span></span> </span></p>

<p><span style="color: rgb(0, 0, 0);">&nbsp;These terms will be discussed a little later.</span></p>

</div>

<div id="objective">
    <h2> Objective </h2>
    <p>The high level goals of the experiment are:
        <ol>
            <li> <b>1:</b> To understand the computation of likelihood of a class, given a sample.</li>
            <li> <b>2:</b> To understand the the use of density/distribution functions to model a class.</li>
            <li> <b>3:</b> To understand the effect of prior probabilities in Bayesian classification.</li>
            <li> <b>4:</b> To understand how two (or more) density functions interact in the feature space to
                         decide a decision boundary between classes.</li>
            <li> <b>4:</b> To understand how the decision boundary varies based on nature of the density functions.</li>
        </ol>
    </p>
</div>


<div id="procedure">
    <h2> Procedure </h2>
    <p> .</p>
    <h3>Stage 1:</h3>
    <ol>
        <li> Set the default parameters and click the generate button; Observe the
        dataset for multiple instances of samples. </li>
        <li> Click the estimate button to compute the mean and covariance of the generated samples;
        Observe the mismatch between the two. Repeat the process for multiple sets of samples generated
        from the same distribution and observe the variations in the estimate</li>
    </ol>

    <h3>Stage 2:</h3>
    <ol>
        <li> Repeat the above procedure for the different values for the number of generated samples. </li>
        <li> Plot a graph between the average error, and number of samples</li>
        <li> Note down your inferences regarding the graph</li>
    </ol>

    <h3>Stage 3:</h3>
    <ol>
        <li> Generate your own dataset by clicking on the plot area. Note that the distribution
        need not be similar to a normal density.</li>
        <li> Estimate the mean and covarance matrices</li>
        <li> Note down your inferences regarding the error committed if you assume the
        distribution to be normal.</li>
    </ol>

</div>

<div id="experiment">
    <h2> Experiment </h2>
    <a href="../experiment/Exp4.jnlp">Click here to open the Experiment</a>
</div>
<div id="quizzes">

    <h2>Quiz</h2>
    <p> Answer the questions provided at the end of the writeup <a href="RandomVariables.pdf">HERE</a>.
    Some of the questions require you to do minimal amount of programming. You are encouraged to
    try out variants of the goals mentioned in the programming questions.
    </p>
</div>

<div id="readings">
    <h2> Further Readings </h2>
    <ul>
        <li><b>*</b> <a href="http://www.rii.ricoh.com/~stork/DHS.html">Pattern Classification</a>,
        by  Duda, Hart and Stork (ed). Second Edition, Wiley, 2001
        </li>
        <li><b>*</b> Article on <a href="http://en.wikipedia.org/wiki/Random_number_generation">Random Number Generation</a> at Wikipedia
        </li>
        <li><b>*</b> Article on <a href="http://en.wikipedia.org/wiki/Pseudorandom_number_generator">Pseudorandom Number Generators</a> at Wikipedia
        </li>
    </ul>
</div>

