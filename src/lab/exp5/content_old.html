<!DOCTYPE html>
<head></head>
<h1 id="main-title">
Bayesian Classification
</h1>

<div id="introduction">
    <h2>Introduction </h2>
    <p> Linear perceptrons allow us to learn a decision boundary that would separate
    two classes. They are very effective when there are only two classes, and they are
    well separated. Such classifiers are referred to as discriminative classifiers.</p>
    <p> In contrast, generative classifiers consider each sample as a random vector, and
    explicity model each class by their distribution or density functions. To carry out
    the classification, we compute the likelihood that a given sample belong to each of the
    candidate classes, and assign the sample to the class that is most likely. In other words,
    we need to compute P(&#969<sub>i</sub>/x) for each class &#969<sub>i</sub>. However, the
    density functions provide only the likelihood of seeing a particular sample, given that
    the sample belongs to a specific class. i.e., the density functions provide us p(x/&#969<sub>i</sub>).
    The Bayes rule provides us with an approach to compute the likelihood of the class for
    a given sample, from the density functions and related information.
</div>

<div id="theory">
    <h2>Theory</h2>

Consider the following quote from a 2000 article in the Economist on the Bayesian Approach 
[<a href="http://www.ai.mit.edu/%7emurphyk/Bayes/economist.html">link</a><span style="color: rgb(0, 0, 0);">]:</span>

<p><i>"The essence of the Bayesian approach is to provide a mathematical rule explaining 
how you should change your existing beliefs in the light of new evidence. In
other words, it allows us to combine new data with their existing knowledge or
expertise. The canonical example is to imagine that a precocious newborn observes 
his first sunset, and wonders whether the sun will rise again or not. He assigns 
equal prior probabilities to both possible outcomes, and represents this by placing
one white and one black marble into a bag. The following day, when
the sun rises, the child places another white marble in the bag. The
probability that a marble plucked randomly from the bag will be white (i.e., the 
child's degree of belief in future sunrises) has thus gone from a half to two-thirds. 
After sunrise the next day, the child adds another white marble, and the probability 
(and thus the degree of belief) goes from two-thirds to three-quarters. And so on. 
Gradually, the initial belief that the sun is just as likely as not to rise each
morning is modified to become a near-certainty that the sun will always
rise." </i></p>

<span style="font-size: 10pt;"><span style="font-family: 'Times New Roman',serif;"></span></span>
<p><span style="color: rgb(0, 0, 0);"><span>&nbsp;</span></span></p>

<p> In terms of classification, the Bayes theorem allows us to combine prior probabilities,
along with observed evidence to arrive at the posterior probability. More or less, conditional
probabilities represent the probability of an event occurring given evidence. To better
understand, Bayes Theorem can be derived from the joint probability of
A and B (i.e. <i>P</i>(<i>A,B</i>)) as
follows:</span></p>
<br>
<p style="text-align: center;"><span style="color: rgb(0, 0, 0);"> <span class="WPCharBoxWrapper" style="width: 217px;"><span class="WPCharBox" style="border: medium none ;"><img src="images/ole.gif" alt="ole.gif" border="0" height="101" width="217"></span></span> </span></p>
<br>

<p><span style="color: rgb(0, 0, 0);">where
</span><span class="serifequation"><span style="color: rgb(0, 0, 0);"><i><span style="font-weight: bold;">P</span></i><span style="font-weight: bold;">(<i>A</i>|<i>B</i>)</span></span></span><span style="color: rgb(0, 0, 0);"> is referred to as the <i>posterior</i>;
<i><span style="font-weight: bold;">P</span></i><span style="font-weight: bold;">(<i>B</i>|<i>A</i>)</span>
is known as the <i>likelihood</i>, <i><span style="font-weight: bold;">P</span></i><span style="font-weight: bold;">(<i>A</i>)</span>
is the <i>prior</i>
and <i><span style="font-weight: bold;">P</span></i><span style="font-weight: bold;">(<i>B</i>)</span>
is generally the <i>evidence</i> and is used as a scaling factor. Therefore, it is handy to
remember Bayes Rule as:</span></p>

<p style="text-align: center;"><span style="color: rgb(0, 0, 0);"> <span class="WPCharBoxWrapper" style="width: 237px;"><span class="WPCharBox" style="border: medium none ;"><img src="images/ole1.gif" alt="ole1.gif" border="0" height="47" width="237"></span></span> </span></p>

<p><span style="color: rgb(0, 0, 0);">&nbsp;These terms will be discussed a little later.</span></p>

</div>

<div id="objective">
    <h2> Objective </h2>
    <p>The high level goals of the experiment are:
        <ol>
            <li> <b>1:</b> To understand the computation of likelihood of a class, given a sample.</li>
            <li> <b>2:</b> To understand the the use of density/distribution functions to model a class.</li>
            <li> <b>3:</b> To understand the effect of prior probabilities in Bayesian classification.</li>
            <li> <b>4:</b> To understand how two (or more) density functions interact in the feature space to
                         decide a decision boundary between classes.</li>
            <li> <b>4:</b> To understand how the decision boundary varies based on nature of the density functions.</li>
        </ol>
    </p>
</div>


<div id="procedure">
    <h2> Procedure </h2>
    <p> .</p>
    <h3>Stage 1:</h3>
    <ol>
        <li> Lauch the experiment and clear the pane. Assign differnt means and covariances for each
         of the classes and observe the resulting densities. Use the mark-all button to observe the
         decision boundaries.</li>
        <li> Note down your observations on the relationship between the decision boundaries and the
         density functions.</li>
    </ol>

    <h3>Stage 2:</h3>
    <ol>
        <li> Repeat the above procedure for different values of prior probabilities.</li>
        <li> Observe the change in the scaled density functions and decision boundaries</li>
        <li> Note down your observations regarding the change of decision boundaries</li>
    </ol>

    <h3>Stage 3:</h3>
    Generate the following types of decision boundaries by varying the means and covariance matrices.
    <ol>
        <li> Straight line </li>
        <li> Parallel Straight lines </li>
        <li> Concentric circles </li>
        <li> Parabola </li>
        <li> Hyperbola </li>
        <li> Four Quadrants </li>
    </ol>
   Explain why these shapes are generated in each case.

</div>

<div id="experiment">
    <h2> Experiment </h2>
    <a href="../experiment/Exp5.jnlp">Click here to open the Experiment</a>
</div>
<div id="quizzes">

    <h2>Quiz</h2>
    <p>
    <b>Q1.</b> The covariance matrix is always
    <br/><br/>
	a. Square<br/>
	b. Positive Semidefinite<br/>
	c. Positive Definite<br/>
	d. Symmetric <br/>
	e. None of the above. <br/>
    <br/><br/>
    For each of the properties you selected, describe what would happen if the covariance
    matrix does not satisfy that property.
    <br/><br/>
    <b>Q2.</b> Describe the possible set of decision boundaries that can be generated using
    gaussian density functions in a two class problem.
    <br/><br/>
    <input type="submit" value="Submit Answers">
    </p>
</div>

<div id="readings">
    <h2> Further Readings </h2>
    <ul>
        <li><b>*</b> <a href="http://www.rii.ricoh.com/~stork/DHS.html">Pattern Classification</a>,
        by  Duda, Hart and Stork (ed). Second Edition, Wiley, 2001
        </li>
        <li><b>*</b> Article on <a href="http://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes Theorem</a> at Wikipedia
        </li>
        <li><b>*</b> Article on <a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes Classifier</a> at Wikipedia
        </li>
    </ul>
</div>

